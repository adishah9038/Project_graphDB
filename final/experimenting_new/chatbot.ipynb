{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8d0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b0fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GOOGLE_API_KEY4\"]\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=1,\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220ea11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    question:str\n",
    "    messages:Annotated[list,add_messages]\n",
    "    loop_count:int\n",
    "    answer:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a59bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available\n",
    "NEO4J_URI=\"neo4j+s://2d5e8539.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"xn8iCGEj2vymA3-43-57qlL63CD70SthzTE_Mt8QfG0\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"2d5e8539\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "enhanced_graph_db = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=\"neo4j\",\n",
    "    password=NEO4J_PASSWORD,\n",
    "    driver_config={\n",
    "        \"max_connection_lifetime\": 300,  # 5 minutes\n",
    "        \"keep_alive\": True,\n",
    "        \"max_connection_pool_size\": 50\n",
    "    },\n",
    "    enhanced_schema=True)\n",
    "\n",
    "graph_db = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=\"neo4j\",\n",
    "    password=NEO4J_PASSWORD,\n",
    "    driver_config={\n",
    "        \"max_connection_lifetime\": 300,  # 5 minutes\n",
    "        \"keep_alive\": True,\n",
    "        \"max_connection_pool_size\": 50\n",
    "    },\n",
    "    enhanced_schema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8409c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Order {ORDERNUMBER: INTEGER, ORDERDATE: STRING, STATUS: STRING, QTR_ID: INTEGER, MONTH_ID: INTEGER, YEAR_ID: INTEGER}\n",
      "Product {PRODUCTCODE: STRING, MSRP: FLOAT, PRODUCTLINE: STRING}\n",
      "Customer {CUSTOMERNAME: STRING, PHONE: STRING, ADDRESSLINE1: STRING, CITY: STRING, POSTALCODE: STRING, COUNTRY: STRING, TERRITORY: STRING, CONTACTLASTNAME: STRING, CONTACTFIRSTNAME: STRING, ADDRESSLINE2: STRING, STATE: STRING}\n",
      "Relationship properties:\n",
      "CONTAINS {PRICEEACH: FLOAT, DEALSIZE: STRING, QUANTITYORDERED: INTEGER, ORDERLINENUMBER: INTEGER, SALES: FLOAT}\n",
      "The relationships:\n",
      "(:Order)-[:CONTAINS]->(:Product)\n",
      "(:Order)-[:PLACED_BY]->(:Customer)\n"
     ]
    }
   ],
   "source": [
    "print(graph_db.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1c3b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.8, max_retries=1, safety_settings={<HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>}, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001705CC4EE90>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'query_tool', 'description': 'This tool can query data from graph database. Query must be in english only.', 'parameters': {'properties': {'query': {}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langraph_neo4j3 import AgentState, run_agent_workflow\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def query_tool(query):\n",
    "    \"\"\"This tool can query data from graph database. Query must be in english only.\"\"\"\n",
    "    state: AgentState = {\n",
    "            \"question\": query,\n",
    "            \"next_action\": \"\",\n",
    "            \"cypher_errors\": [],\n",
    "            \"database_records\": [],\n",
    "            \"steps\": [],\n",
    "            \"answer\": \"\",\n",
    "            \"cypher_statement\": \"\"\n",
    "        }\n",
    "    result = run_agent_workflow(state,enhanced_graph_db)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "tools=[query_tool]\n",
    "llm_with_tool=llm.bind_tools(tools)\n",
    "llm_with_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7832bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The question will not be a direct question. \n",
    "# 2. You need to identify the target & the problem from the question first.\n",
    "# 3. Then find all the possible data sources i.e. nodes and relations affecting the target with the problem using the graph schema provided.\n",
    "# 4. Then develop a probable reason (hypothesis) for the causing the problem and verify the contribution of that reason in causing the problem using the data queried with the query_tool.\n",
    "# 5. Step by step you will have to generate hypothesis and verify it using query_tool. Improvise results with each step and reach conclusion.\n",
    "\n",
    "# Note\n",
    "# 1. query_tool accepts clear and specific instructions to query the graph in plane english language only, donot generate cypher by yourself.\n",
    "# 2. Try to get as much data as possible in one single query_tool using words like 'and', 'respectively' etc.\n",
    "# 3. You can only use the query_tool 6 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f480473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "\"system\",\n",
    "\"\"\"\n",
    "You are a expert data analyst. Your job is to do the root cause analysis for the provided question. \n",
    "You will be provided with a graph database schema and a graph query_tool to query the data from graph database. \n",
    "Note:\n",
    "1. query_tool accepts instructions in english language only.\n",
    "\"\"\"\n",
    "            ),\n",
    "            (\n",
    "\"human\",\n",
    "\"\"\"\n",
    "### Graph database schema (Use it for understanding relations)\n",
    "---\n",
    "{schema}\n",
    "---\n",
    "### Conversation History:\n",
    "---\n",
    "{conversation}\n",
    "---\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "chain = prompt | llm_with_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3c29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1706d6d5550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [chain.invoke({\"schema\":graph_db.schema,\"conversation\":state['messages'], \"question\": state['question'],\"loop_count\":state[\"loop_count\"]+1})]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58a4a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1706d6d5550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\"You job is to summarize the conversation and frame an answer for the main question asked in the conversation.\"),\n",
    "            (\"human\",\"{conversation}\")\n",
    "        ]\n",
    "    )\n",
    "summary_chain = summary_prompt | llm\n",
    "def summary(state: State): \n",
    "    return {\"answer\":summary_chain.invoke({\"conversation\":state['messages']})}\n",
    "\n",
    "graph_builder.add_node(\"summary\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b24a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1706d6d5550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "tool_node = BasicToolNode(tools=[query_tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad468881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(state: State,):\n",
    "    if state['loop_count']>6:\n",
    "        return \"summary\"\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6029d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", END: END, \"summary\":\"summary\"},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"summary\",END)\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc7c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Processing user input: Why is the sale decreasing from 2004 to 2005?\n",
      "\n",
      "üîç Querying... 'Get total sales for each year from 2004 to 2005'\n",
      "üîç Querying... 'Get the count of orders for each status in 2004'\n",
      "üîç Querying... 'Get the count of orders for each status in 2005'\n",
      "ü§ñ AI: The significant decrease in sales from 2004 to 2005 can be attributed to a sharp decline in the volume of successfully completed orders.\n",
      "\n",
      "Here's a breakdown of the contributing factors:\n",
      "\n",
      "*   **Reduced Number of Successful Orders:** In 2004, a total of 140 orders were successfully completed (139 Shipped and 1 Resolved), directly contributing to the sales figures. In contrast, 2005 saw a drastic reduction to only 47 orders reaching a successful status (45 Shipped and 2 Resolved). This substantial drop in completed transactions directly impacts total sales.\n",
      "*   **Overall Decrease in Orders:** The total number of orders placed also saw a significant decline from 144 in 2004 to 59 in 2005. Fewer orders being initiated naturally leads to lower overall sales.\n",
      "*   **Higher Proportion of Unresolved Orders:** While not directly reducing sales, 2005 experienced a higher percentage of orders in statuses such as 'Disputed', 'In Process', or 'On Hold' (12 out of 59 orders, approximately 20.3%) compared to 2004 (4 out of 144 orders, approximately 2.7%). This indicates that a larger proportion of orders in 2005 did not convert into revenue-generating sales.\n",
      "\n",
      "These combined factors, primarily the significant reduction in the number of orders that were shipped or resolved, explain the observed decline in sales from 2004 to 2005.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "üß© Final Agent State Keys: ['messages']\n",
      "üí¨ Final Answer: No answer found\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    initial_state = {\n",
    "        \"question\": user_input,\n",
    "        \"messages\": [],\n",
    "        \"loop_count\": 0,\n",
    "        \"answer\": \"\",\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüß† Processing user input: {user_input}\\n\")\n",
    "\n",
    "    final_state = None\n",
    "\n",
    "    for event in graph.stream(initial_state):\n",
    "        for value in event.values():\n",
    "            final_state = value\n",
    "            messages = value.get(\"messages\", [])\n",
    "            if not messages:\n",
    "                continue\n",
    "\n",
    "            last_msg = messages[-1]\n",
    "            role = getattr(last_msg, \"type\", \"ai\")\n",
    "            content = getattr(last_msg, \"content\", \"\")\n",
    "\n",
    "            # üßç User messages\n",
    "            if role in [\"human\", \"user\"]:\n",
    "                print(f\"üßç‚Äç‚ôÇÔ∏è User: {content}\")\n",
    "\n",
    "            # ü§ñ AI messages (can include tool calls)\n",
    "            elif role in [\"ai\", \"assistant\"]:\n",
    "                if content.strip():\n",
    "                    print(f\"ü§ñ AI: {content}\")\n",
    "                else:\n",
    "                    tool_calls = getattr(last_msg, \"tool_calls\", None)\n",
    "                    if tool_calls:\n",
    "                        for call in tool_calls:\n",
    "                            tool_args = call.get(\"args\", {})\n",
    "                            query_text = tool_args.get(\"query\", \"\").strip()\n",
    "                            if query_text:  # ‚úÖ only print if query exists\n",
    "                                print(f\"üîç Querying... '{query_text}'\")\n",
    "                    # skip if no tool_calls or empty args ‚Äî don‚Äôt print anything\n",
    "\n",
    "            # üß∞ Tool message (rarely used here, but safe fallback)\n",
    "            elif role == \"tool\":\n",
    "                tool_args = getattr(last_msg, \"args\", {})\n",
    "                query_text = tool_args.get(\"query\", \"\").strip()\n",
    "                if query_text:\n",
    "                    print(f\"üîç Querying... '{query_text}'\")\n",
    "\n",
    "            else:\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "    return final_state\n",
    "\n",
    "\n",
    "# Example usage\n",
    "user_input = \"Why is the sale decreasing from 2004 to 2005?\"\n",
    "final_state = stream_graph_updates(user_input)\n",
    "\n",
    "print(\"üß© Final Agent State Keys:\", list(final_state.keys()))\n",
    "print(\"üí¨ Final Answer:\", final_state.get(\"answer\", \"No answer found\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
