{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0e8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6902cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GOOGLE_API_KEY4\"]\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=1,\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf70598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages:Annotated[list,add_messages]\n",
    "    loop_count:int\n",
    "    answer:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddadee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available\n",
    "NEO4J_URI=\"neo4j+s://2d5e8539.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"xn8iCGEj2vymA3-43-57qlL63CD70SthzTE_Mt8QfG0\"\n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "AURA_INSTANCEID=\"2d5e8539\"\n",
    "AURA_INSTANCENAME=\"Instance01\"\n",
    "\n",
    "\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "enhanced_graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=\"neo4j\",\n",
    "    password=NEO4J_PASSWORD,\n",
    "    driver_config={\n",
    "        \"max_connection_lifetime\": 300,  # 5 minutes\n",
    "        \"keep_alive\": True,\n",
    "        \"max_connection_pool_size\": 50\n",
    "    },\n",
    "    enhanced_schema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e50f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langraph_neo4j3 import AgentState, run_agent_workflow\n",
    "\n",
    "def alpha_tool(query):\n",
    "    \"\"\"\n",
    "    This tool executes a factual query on a Neo4j graph database.\n",
    "    \"\"\"\n",
    "    Agentstate: AgentState = {\n",
    "            \"question\": query,\n",
    "            \"next_action\": \"\",\n",
    "            \"cypher_errors\": [],\n",
    "            \"database_records\": [],\n",
    "            \"steps\": [],\n",
    "            \"answer\": \"\",\n",
    "            \"cypher_statement\": \"\"\n",
    "        }\n",
    "    result = run_agent_workflow(Agentstate,enhanced_graph)\n",
    "    return f\"For query '{query}', result: {result['answer']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd9a87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.5, max_retries=1, safety_settings={<HarmCategory.HARM_CATEGORY_HARASSMENT: 7>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_HATE_SPEECH: 8>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 9>: <HarmBlockThreshold.BLOCK_NONE: 4>, <HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>}, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000218A6975650>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'alpha_tool', 'description': 'This tool executes a factual query on a Neo4j graph database.', 'parameters': {'properties': {'query': {}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=[alpha_tool]\n",
    "llm_with_tool=llm.bind_tools(tools)\n",
    "llm_with_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d99721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from typing import Literal\n",
    "# from pydantic import BaseModel, Field\n",
    "# guard_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"system\",\n",
    "#                 \"\"\"\n",
    "\n",
    "# You are a conversational agent with reasoning and data interpretation capabilities, working with a graph database.\n",
    "# Every use input can be classified into 3 modes: Conversation, Query and Hypothesis.\n",
    "#  1. If the question is unrelated to the graph or can be answered without querying graph, reply normally (no tool use). (Conversation)\n",
    "#  2. If the question can be answered with a single direct query â†’ call query_tool ONCE with a plain English instruction. (Query)\n",
    "#  3. If the question is indirect or requires reasoning, i.e. iterative hypothesis generation and querying to verify its' correctness. (Hypothesis)\n",
    "\n",
    "# ### Schema\n",
    "# ---\n",
    "# {schema}\n",
    "# ---\n",
    "\n",
    "# Response must be in JSON format with 2 fields - 'Response' and 'Mode'.\n",
    "# Simple response for Conversation mode, plain english instruction for Query mode and empty string in case of Hypothesis mode.\n",
    "# \"\"\"\n",
    "#             ),\n",
    "#             (\n",
    "#                 \"human\",\n",
    "#         \"\"\"User Question:\n",
    "#     {question}\n",
    "#     \"\"\"\n",
    "#             ),\n",
    "#         ]\n",
    "#     )\n",
    "# class guardOutput(BaseModel):\n",
    "#     Mode: Literal[\"Conversation\", \"Query\", \"Hypothesis\"] = Field(\n",
    "#         description=\"Decision on whether the question is related to data\"\n",
    "#     )\n",
    "#     Response: str = Field(\n",
    "#         description=\"LLM response.\"\n",
    "#     )\n",
    "    \n",
    "# guard_chain = guard_prompt | llm.with_structured_output(guardOutput)\n",
    "# response = guard_chain.invoke({\"schema\":enhanced_graph.schema, \"question\":\"Why are my customers declining?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f3cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "\"You are performing root cause analysis. You may call alpha_tool multiple times to gather data.\n",
    "Continue reasoning until you have sufficient evidence to explain the root cause.\n",
    "After each tool response, reassess and decide whether another query is needed.\"\n",
    "\n",
    "## Schema Reference:\n",
    "{schema}\n",
    "Understand the nodes and relations from the above schema and leverage it to make a hypothesis for the root cause analysis. \n",
    "\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "User Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "chain = prompt | llm_with_tool\n",
    "\n",
    "#    - Form a hypothesis for the root question.\n",
    "#    - Iteratively call query_tool to test hypotheses.\n",
    "#    - Refine results step by step.\n",
    "#    - Then summarize findings.\n",
    "\n",
    "# Always:  \n",
    "# - Never generate Cypher.  \n",
    "# - Use plain English instructions with `query_tool`.  \n",
    "# - Use the schema above to understand what entities and relationships are available before deciding whether to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d76013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",\"You job is to summarize the conversation and frame an answer for the question asked.\"),\n",
    "            (\"human\",\"{conversation}\")\n",
    "        ]\n",
    "    )\n",
    "summary_chain = summary_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76558eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stategraph\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# def guard_condition(state: State,) -> Literal[]:\n",
    "    \n",
    "#     response = guard_chain.invoke({\"schema\":enhanced_graph.schema, \"question\":\"Why are my customers declining?\"})\n",
    "#     if response.Mode == \"Conversation\":\n",
    "#         return {\"messages\":AIMessage[response.Response]}\n",
    "#     elif response.Mode == \"Query\":\n",
    "#         return {\"messages\":ToolMessage[query_tool(response.Response)]}\n",
    "#     elif response.Mode == \"Hypothesis\":\n",
    "#         return {\"messages\":AIMessage[\"Hypothesis mode...\"]}\n",
    "    \n",
    "## Node definition\n",
    "def tool_calling_llm(state: State):\n",
    "    # Increment loop\n",
    "    state[\"loop_count\"] += 1\n",
    "\n",
    "    # Extract latest user question only\n",
    "    user_question = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    # Invoke your LLM chain\n",
    "    ai_msg = chain.invoke({\n",
    "        \"question\": user_question,\n",
    "        \"schema\": enhanced_graph.schema,\n",
    "    })\n",
    "\n",
    "    # Return as a list (so Annotated[add_messages] will append it)\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "def summarize_llm(state:State):\n",
    "    state[\"answer\"] = summary_chain.invoke({\"conversation\":state[\"messages\"]})\n",
    "    return state\n",
    "\n",
    "## Graph\n",
    "builder=StateGraph(State)\n",
    "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "builder.add_node(\"tools\",ToolNode(tools))\n",
    "builder.add_node(\"summarize\", summarize_llm)\n",
    "\n",
    "## Add Edges\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "def reasoning_condition(state: State):\n",
    "    # If tool call found, go to tools\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    if isinstance(last_msg, AIMessage) and getattr(last_msg, \"tool_calls\", None):\n",
    "        return \"tools\"\n",
    "    \n",
    "    # If loop count < 4, keep reasoning\n",
    "    if state.get(\"loop_count\", 0) < 4:\n",
    "        return \"tool_calling_llm\"\n",
    "    \n",
    "    # Else summarize and stop\n",
    "    return \"summarize\"\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    reasoning_condition  # This function chooses either \"tools\", END, or \"summarize\"\n",
    ")\n",
    "builder.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "builder.add_edge(\"summarize\", END)\n",
    "\n",
    "## compile the graph\n",
    "graph=builder.compile()\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf37bf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004', result: The total sales for 2004 were 4,724,162.60.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2003 RETURN sum(c.SALES) AS Sales2003\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2003 RETURN sum(c.SALES) AS Sales2003', result: The total sales for the year 2003 were 3,516,979.54.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2002 RETURN sum(c.SALES) AS Sales2002\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004', result: The total sales for 2004 were 4,724,162.60.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2003 RETURN sum(c.SALES) AS Sales2003\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2005 RETURN sum(c.SALES) AS Sales2005', result: The total sales for the year 2005 were 1,791,486.71.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004', result: The total sales for 2004 were 4,724,162.60.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2003 RETURN sum(c.SALES) AS Sales2003\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2005 RETURN sum(c.SALES) AS Sales2005', result: The total sales for the year 2005 were 1,791,486.71.\n",
      "ðŸ¤– Tool Call - Query: MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004\n",
      "ðŸ§° Tool Response - For query 'MATCH (o:Order)-[c:CONTAINS]->() WHERE o.YEAR_ID = 2004 RETURN sum(c.SALES) AS Sales2004', result: The total sales for the year 2004 were 4,724,162.60.\n",
      "ðŸ¤– AI - The total sales for 2004 are 4,724,162.60. What specific problem or anomaly are you investigating with this sales figure? Please provide more context so I can help you with the root cause analysis.\n",
      "ðŸ¤– AI - I need more information to understand what specific problem or anomaly you're investigating with the 2004 sales figure of 4,724,162.60.\n",
      "\n",
      "Please tell me:\n",
      "\n",
      "*   **What was the expected sales figure for 2004?** (e.g., target, previous year's sales, forecast)\n",
      "*   **Is this figure higher or lower than expected? By how much?**\n",
      "*   **Are there any other related metrics or observations that seem unusual for 2004?** (e.g., specific product lines underperforming, particular customer segments showing reduced activity, changes in order volume or average order value)\n",
      "*   **What is the business impact of this deviation?**\n",
      "\n",
      "Once I have a clearer understanding of the problem, I can start to formulate hypotheses and use the `alpha_tool` to query the database for potential root causes.\n",
      "ðŸ¤– Tool Call - Query: \n",
      "MATCH (o:Order)-[c:CONTAINS]->(:Product)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, SUM(c.SALES) AS TotalSales\n",
      "ORDER BY Year\n",
      "\n",
      "ðŸ§° Tool Response - For query '\n",
      "MATCH (o:Order)-[c:CONTAINS]->(:Product)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, SUM(c.SALES) AS TotalSales\n",
      "ORDER BY Year\n",
      "', result: For the year 2003, the total sales were 3,516,979.54. For the year 2004, the total sales were 4,724,162.60.\n",
      "ðŸ¤– Tool Call - Query: \n",
      "MATCH (o:Order)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, COUNT(o) AS NumberOfOrders\n",
      "ORDER BY Year\n",
      "\n",
      "ðŸ§° Tool Response - For query '\n",
      "MATCH (o:Order)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, COUNT(o) AS NumberOfOrders\n",
      "ORDER BY Year\n",
      "', result: In 2003, there were 104 orders. In 2004, there were 144 orders.\n",
      "ðŸ¤– Tool Call - Query: \n",
      "MATCH (o:Order)-[:CONTAINS]->(p:Product)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, SUM(r.SALES) AS TotalSales\n",
      "ORDER BY Year\n",
      "\n",
      "ðŸ§° Tool Response - For query '\n",
      "MATCH (o:Order)-[:CONTAINS]->(p:Product)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, SUM(r.SALES) AS TotalSales\n",
      "ORDER BY Year\n",
      "', result: The total sales for 2003 were 3,516,979.54, and for 2004, the total sales were 4,724,162.60.\n",
      "ðŸ¤– Tool Call - Query: \n",
      "MATCH (o:Order)-[:CONTAINS]->(r:CONTAINS)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, o.QTR_ID AS Quarter, SUM(r.SALES) AS QuarterlySales\n",
      "ORDER BY Year, Quarter\n",
      "\n",
      "ðŸ§° Tool Response - For query '\n",
      "MATCH (o:Order)-[:CONTAINS]->(r:CONTAINS)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, o.QTR_ID AS Quarter, SUM(r.SALES) AS QuarterlySales\n",
      "ORDER BY Year, Quarter\n",
      "', result: The quarterly sales for 2003 and 2004 are as follows:\n",
      "\n",
      "*   **2003:**\n",
      "    *   Quarter 1: $445,094.69\n",
      "    *   Quarter 2: $562,365.22\n",
      "    *   Quarter 3: $649,514.54\n",
      "    *   Quarter 4: $1,860,005.09\n",
      "*   **2004:**\n",
      "    *   Quarter 1: $833,730.68\n",
      "    *   Quarter 2: $766,260.73\n",
      "    *   Quarter 3: $1,109,396.27\n",
      "    *   Quarter 4: $2,014,774.92\n",
      "ðŸ¤– Tool Call - Query: \n",
      "MATCH (o:Order)-[:CONTAINS]->(c:CONTAINS)-[:CONTAINS]->(p:Product)\n",
      "WHERE o.YEAR_ID IN [2003, 2004]\n",
      "RETURN o.YEAR_ID AS Year, o.QTR_ID AS Quarter, p.PRODUCTLINE AS ProductLine, SUM(c.SALES) AS QuarterlySales\n",
      "ORDER BY Year, Quarter, ProductLine\n",
      "\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, AIMessage, ToolMessage\n\u001b[0;32m      3\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy did my sale drop from 2005 to 2004?\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloop_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_msg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32me:\\PwC_Pocs\\GenAI\\Project_1\\.venv_11\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2456\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2448\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m   2449\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   2450\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2454\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[0;32m   2455\u001b[0m     )\n\u001b[1;32m-> 2456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[0;32m   2457\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[0;32m   2458\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[1;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Why did my sale drop from 2005 to 2004?\")],\n",
    "    \"loop_count\": 0,\n",
    "    \"answer\": \"\"\n",
    "}\n",
    "\n",
    "for event in graph.stream(initial_state):\n",
    "    for value in event.values():\n",
    "        last_msg = value[\"messages\"][-1]\n",
    "\n",
    "        if isinstance(last_msg, AIMessage):\n",
    "            if getattr(last_msg, \"tool_calls\", None):  \n",
    "                tool_call = last_msg.tool_calls[0]\n",
    "                print(f\"ðŸ¤– Tool Call - Query: {tool_call['args']['query']}\")\n",
    "            else:\n",
    "                # Extract content safely\n",
    "                if isinstance(last_msg.content, list):\n",
    "                    content_text = \" \".join(\n",
    "                        part.get(\"text\", \"\") for part in last_msg.content if isinstance(part, dict)\n",
    "                    )\n",
    "                else:\n",
    "                    content_text = last_msg.content\n",
    "                print(f\"ðŸ¤– AI - {content_text}\")\n",
    "\n",
    "        elif isinstance(last_msg, ToolMessage):\n",
    "            print(f\"ðŸ§° Tool Response - {last_msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
